{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this part\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import clustering, regression, evaluation\n",
    "from pyspark.ml import clustering \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark.sql.functions import col, expr, when\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the netflow data\n",
    "netflow = spark.read.csv('netflow.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest_ip',\n",
       " 'dest_port',\n",
       " 'src_ip',\n",
       " 'src_port',\n",
       " 'app',\n",
       " 'bytes',\n",
       " 'bytes_in',\n",
       " 'bytes_out',\n",
       " 'timestamp',\n",
       " 'endtime',\n",
       " '_indextime',\n",
       " '_raw',\n",
       " '_sourcetype',\n",
       " '_subsecond',\n",
       " '_time']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not need the last 3 columns because it is Splunk add-ons\n",
    "netflow_tcp = netflow.select(netflow.columns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of distinct destination IP addresses in the dataset\n",
    "netflow_tcp.select(\"dest_ip\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of distinct dest_ip dest_port combinations\n",
    "netflow_tcp.select(\"dest_ip\", \"dest_port\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of distinct source IP addresses in the dataset\n",
    "netflow_tcp.select(\"src_ip\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of distinct src_ip src_port combinations\n",
    "netflow_tcp.select(\"src_ip\", \"src_port\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets identify how many IP Addresses are in the dest_ip range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_tcp.groupBy('dest_ip').count().show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return shows there is over 200.  Some appear often, othe appear only once.  Will break up by hour and then calculate by hour the distribution of connections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Web Upload & Total Bytes z-score Distribution\n",
    "#### Steps\n",
    "- 1 develop a user defined function (udf)\n",
    "- 2 calcualte mean and standard deviation\n",
    "- 3 run the udf withColumn bytes calcualted\n",
    "- 4 classify all rows where z-score is higher than 2.58 or less than -2.58\n",
    "- 5 classify all rows where total bytes is larger than 35MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col\n",
    "#mean total bytes\n",
    "mean = netflow_tcp.select(mean(col(\"bytes\"))).alias(\"mean\").collect()[0]\n",
    "#standard deviation\n",
    "stdev = netflow_tcp.select(stddev(\"bytes\")).alias(\"sdev\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#compute the z-score for each total bytes\n",
    "def z_score(col):\n",
    "    #average = mean(col)\n",
    "    #standard_dev = stddev(col)\n",
    "    #score = np.absolute((col - mean[0]) / stdev[0])\n",
    "    return (col - mean[0]) / stdev[0]\n",
    "z_udf = udf(z_score)\n",
    "\n",
    "netflow_z = netflow_tcp.withColumn(\"z_score\", z_udf(\"bytes\"))\n",
    "#netflow_z = netflow_z.select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify z-score higher than 2.58 as 1 else 0 => 2.58 is z_score between 3 and 3 standard deviations.  \n",
    "netflow_z = netflow_z.select(col(\"*\"), fn.expr(\"case when z_score > 2.58 OR z_score < -2.58 then 1 else 0 end\").alias(\"z_score_class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------------+--------+-----------+-------------+\n",
      "|       dest_ip|dest_port|      src_ip|src_port|        app|z_score_class|\n",
      "+--------------+---------+------------+--------+-----------+-------------+\n",
      "| 52.42.208.228|     3306|172.31.10.10|   42328|        ssl|            1|\n",
      "|  172.31.4.249|     3306|52.40.10.231|   41479|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   41479|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   37954|        ssl|            1|\n",
      "|199.117.103.25|       80|  10.0.2.101|   56914|        cnn|            1|\n",
      "|199.117.103.33|       80|  10.0.2.101|   56753|        cnn|            1|\n",
      "|173.222.45.139|      443|  10.0.2.101|   56522|     sizmek|            1|\n",
      "|173.194.54.218|       80|  10.0.2.101|   56041|google_play|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   42328|        ssl|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   41479|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   37954|        ssl|            1|\n",
      "|199.117.103.33|       80|  10.0.2.101|   56753|        cnn|            1|\n",
      "|173.222.45.139|      443|  10.0.2.101|   56522|     sizmek|            1|\n",
      "|173.194.54.218|       80|  10.0.2.107|   62366|google_play|            1|\n",
      "|173.194.54.218|       80|  10.0.2.101|   56041|google_play|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   43182|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   43178|      mysql|            1|\n",
      "|173.222.45.139|      443|  10.0.2.101|   56522|     sizmek|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   43178|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   37954|        ssl|            1|\n",
      "+--------------+---------+------------+--------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check to see which dest_port and src_ip combinations have a z_score_class of 1\n",
    "netflow_z.select(\"dest_ip\", \"dest_port\", \"src_ip\", \"src_port\", \"app\", \"z_score_class\").where(\"z_score_class == 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns 5198 values where the z_score is greater than 2.58... or 99% confidence interval or greater. Additionally, looking at the traffic above, the IP address 52.42.208.228 is the **destination** and is using port 3306, which is standard for MYSQL with SSL or other encryption  \n",
    "\n",
    "App sizmek is an amazon company which is used for advertising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to understand the ports and IP addresses with z_score_class of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_z.select(\"dest_ip\", \"dest_port\", \"src_ip\", \"src_port\", \"app\", \"z_score_class\").where(\"z_score_class == 1\").groupBy(\"dest_ip\",\"dest_port\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The port grouping shows IP address 172.31.10.10 connecting on 10001 and 443.  Port 10001 is used by protocol SCP and used for online gaming, trojans, backdoors, and Network Attached Storage (NAS).  The IP Address is in the private range and not routeable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_z.select(\"dest_ip\", \"dest_port\", \"src_ip\", \"src_port\", \"app\", \"z_score_class\").where(\"z_score_class == 1\").groupBy(\"dest_ip\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the distribution of IP addresses that have anomolous bytes size based on z_score > 2.58 OR < -2.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Web Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of records where the value is greater than 35MB\n",
    "netflow_z.where('bytes > 35000000').count().show() # returns 78 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label all the traffic with large web upload.\n",
    "netflow_z_up = netflow_z.select(\"*\", when(col(\"bytes\") > 35000000, 1).otherwise(0).alias(\"large_upload\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label TOR traffic\n",
    "#### Steps\n",
    "Tor typically uses TCP ports 9001, 9030, 9150\n",
    "- 1 find all dest and src ports where ports match with TOR traffic\n",
    "- 2 label the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_spam2_df = sms_spam_df.select('text', when(col(\"type\") == 'spam', 1.0).otherwise(0).alias('type'))\n",
    "netflow_z.where('dest_port == 9001 OR dest_port == 9030 OR dest_port == 9150 OR dest_port == 31337').count() #returns 28 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_z.where('src_port == 9001 OR src_port == 9030 OR src_port == 9150 OR src_port == 31337').count() #returns 8 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column for all flows that meet the case of meeting one or more of the tor port conditions... where if yes then 1, otherwise 0.\n",
    "\n",
    "netflow_z_up = netflow_z_up.select(col(\"*\"), fn.expr(\"case when dest_port == 9001 OR dest_port == 9030 OR dest_port == 9150 OR dest_port == 31337 OR src_port == 9001 OR src_port == 9030 OR src_port == 9150 OR src_port == 31337 then 1 else 0 end\").alias(\"tor_class\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Difference error Distribution\n",
    "#### Steps\n",
    "- 1 create a new column with the diff of timestamp and endtime\n",
    "- 2 get the mean and standard deviation of the time\n",
    "- 3 calculate the z-score for the time\n",
    "- 4 label all times that are outliers (+- 2.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second, mean, unix_timestamp, substring, abs\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as func\n",
    "from scipy import stats\n",
    "from datetime import timedelta\n",
    "#newDF = df.orderBy(unix_timestamp(df(\"stringCol\"), pattern).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the dataframe by 'timestamp'\n",
    "netflow_z = netflow_z_up.orderBy(unix_timestamp(netflow_z[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timestamp', 'timestamp'),\n",
       " ('endtime', 'timestamp'),\n",
       " ('z_score', 'string'),\n",
       " ('z_score_class', 'int'),\n",
       " ('large_upload', 'int'),\n",
       " ('tor_class', 'int')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_z.dtypes[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Diff\n",
    "timeFmt = \"yyyy-MM-dd' 'HH:mm:ss.SSS\"\n",
    "#netflow_time_diff = netflow_z_up\\\n",
    "netflow_time_diff = netflow_z\\\n",
    "    .withColumn('start_msec', unix_timestamp('timestamp', format=timeFmt) + substring('timestamp', -3, 3).cast('float')/1000)\\\n",
    "    .withColumn('end_msec', unix_timestamp('endtime', format=timeFmt) + substring('endtime', -3, 3).cast('float')/1000)\\\n",
    "    .withColumn('diff_msec', (abs(col('start_msec') - col('end_msec')).cast('double')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col\n",
    "\n",
    "#get the mean of diff time\n",
    "#mean total bytes\n",
    "mean_msec = netflow_time_diff.select(mean(col(\"diff_msec\"))).alias(\"mean_msec\").collect()[0]\n",
    "#get the standard deviation of diff time\n",
    "#standard deviation\n",
    "stdev_msec = netflow_time_diff.select(stddev(\"diff_msec\")).alias(\"stdev_msec\").collect()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 299.23850464502044 standard dev: 613.9616176450414\n"
     ]
    }
   ],
   "source": [
    "print(\"mean:\", mean_msec[0], \"standard dev:\", stdev_msec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#create a user defined function for the diff time\n",
    "def z_score_time(col):\n",
    "    return (col - mean_msec[0]) / stdev_msec[0]\n",
    "z_udf = udf(lambda z: z_score_time(z), DoubleType())\n",
    "\n",
    "netflow_time = netflow_time_diff.select('*', z_udf(\"diff_msec\").alias('z_score_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|z_score_time        |\n",
      "+--------------------+\n",
      "|60.36249951593672   |\n",
      "|60.36249951593672   |\n",
      "|60.36249951593672   |\n",
      "|60.36249951593672   |\n",
      "|-0.21945590481709806|\n",
      "|0.31720629813789736 |\n",
      "|-0.2111035916665231 |\n",
      "|-0.38229995792985855|\n",
      "|-0.47661529859911056|\n",
      "|-0.09770236778612007|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_time.select(\"z_score_time\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o360.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 816, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ed5c86c25cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetflow_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z_score_time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z_score_time > 2.58\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o360.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 816, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#####ERRRRROOOORRRRR ######\n",
    "#this does not work because ... well... I just cannot figure out why it does not work. Something to do with unsupported type\n",
    "#TypeError: unsupported operand type(s) for -: 'NoneType' and 'float' -> referencing z_score_time\n",
    "\n",
    "netflow_time.select('z_score_time').where(\"z_score_time > 2.58\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2036985"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull\n",
    "#df.where(df.a.isNull()).count()\n",
    "netflow_time_diff.select(isnull('time_z_score')).count()\n",
    "\n",
    "\n",
    "#.where(netflow_time_diff.z_score_time.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another techinque to cast the time as a Double type\n",
    "from pyspark.sql.types import DoubleType\n",
    "netflow_time_diff = netflow_time_diff.withColumn(\"time_z_score\", netflow_time_diff[\"z_score_time\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        time_z_score|\n",
      "+--------------------+\n",
      "|   60.36249909441595|\n",
      "|   60.36249909441595|\n",
      "|   60.36249909441595|\n",
      "|   60.36249909441595|\n",
      "|-0.21945589706690755|\n",
      "|  0.3172062993391025|\n",
      "| -0.2111035985269344|\n",
      "| -0.3822999584602314|\n",
      "| -0.4766153069898921|\n",
      "|  -0.097702359462893|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_time_diff.select(\"time_z_score\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify z-score higher than 2.58 as 1 else 0 => 2.58 is z_score between 3 and 3 standard deviations.  \n",
    "netflow_df = netflow_time_diff.select(col(\"*\"), fn.expr(\"case when z_score_time > 2 OR z_score_time < -2 then 1 else 0 end\").alias(\"z_score_time_class\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New IP Address connection from private subnet\n",
    "#### Steps\n",
    "- 1 Identify Private IP addresses\n",
    "- 2 Calculate IP addresses per hour\n",
    "- 3 If IP address per hour is new, label it anomolous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the dataframe by 'timestamp'\n",
    "netflow_z = netflow_time_diff.orderBy(unix_timestamp(netflow_z[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_pd = netflow_z.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>app</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>endtime</th>\n",
       "      <th>z_score</th>\n",
       "      <th>diff_time_ms</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0.1.100</td>\n",
       "      <td>135</td>\n",
       "      <td>10.0.1.1</td>\n",
       "      <td>39814</td>\n",
       "      <td>msrpc</td>\n",
       "      <td>1470</td>\n",
       "      <td>838</td>\n",
       "      <td>632</td>\n",
       "      <td>2017-08-01 18:29:25.404</td>\n",
       "      <td>2017-08-01 18:29:35.335</td>\n",
       "      <td>-0.024271898960842864</td>\n",
       "      <td>00:00:09.931000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.31.10.10</td>\n",
       "      <td>443</td>\n",
       "      <td>52.34.175.192</td>\n",
       "      <td>60761</td>\n",
       "      <td>ssl</td>\n",
       "      <td>5045</td>\n",
       "      <td>1926</td>\n",
       "      <td>3119</td>\n",
       "      <td>2017-08-01 18:31:09.521</td>\n",
       "      <td>2017-08-01 18:22:16.052</td>\n",
       "      <td>-0.023062881596103615</td>\n",
       "      <td>00:08:53.469000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dest_ip  dest_port         src_ip  src_port    app  bytes  bytes_in  \\\n",
       "0    10.0.1.100        135       10.0.1.1     39814  msrpc   1470       838   \n",
       "1  172.31.10.10        443  52.34.175.192     60761    ssl   5045      1926   \n",
       "\n",
       "   bytes_out               timestamp                 endtime  \\\n",
       "0        632 2017-08-01 18:29:25.404 2017-08-01 18:29:35.335   \n",
       "1       3119 2017-08-01 18:31:09.521 2017-08-01 18:22:16.052   \n",
       "\n",
       "                 z_score    diff_time_ms private  \n",
       "0  -0.024271898960842864 00:00:09.931000       0  \n",
       "1  -0.023062881596103615 00:08:53.469000     NaN  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through dataframe... if private to private match, the return 0, else will return NaN \n",
    "netflow_pd.loc[(netflow_pd.dest_ip.str.contains('10.0|172.31|192.168')) & (netflow_pd.src_ip.str.contains('10.0|172.31|192.168')), 'private'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all values of NaN with 1 meaning 1 is not a private to private mapping\n",
    "netflow_pd['private'] = netflow_pd.private.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>app</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>endtime</th>\n",
       "      <th>z_score</th>\n",
       "      <th>z_score_class</th>\n",
       "      <th>large_upload</th>\n",
       "      <th>tor_class</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.97.128.34</td>\n",
       "      <td>443</td>\n",
       "      <td>10.0.2.109</td>\n",
       "      <td>58107</td>\n",
       "      <td>office365</td>\n",
       "      <td>1441608</td>\n",
       "      <td>167497</td>\n",
       "      <td>1274111</td>\n",
       "      <td>2017-07-31 14:46:43.264</td>\n",
       "      <td>2017-08-01 01:09:22.761</td>\n",
       "      <td>0.46276358345058344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dest_ip  dest_port      src_ip  src_port        app    bytes  \\\n",
       "0  40.97.128.34        443  10.0.2.109     58107  office365  1441608   \n",
       "\n",
       "   bytes_in  bytes_out               timestamp                 endtime  \\\n",
       "0    167497    1274111 2017-07-31 14:46:43.264 2017-08-01 01:09:22.761   \n",
       "\n",
       "               z_score  z_score_class  large_upload  tor_class  private  \n",
       "0  0.46276358345058344              0             0          0      1.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if the non_private \n",
    "netflow_pd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pandas dataframe locally\n",
    "netflow_pd.to_csv('netflow_data.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9849bc6400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create historgram showing count of private to private and not\n",
    "netflow_pd['private'].value_counts().plot(kind='bar')\n",
    "#we can see there is a significant amount of non private to private communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataframe back to a spark, this will contain a new data_frame that has a private column \n",
    "netflow_data = spark.createDataFrame(netflow_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the counts \n",
    "aggs_dest_test = netflow_data.groupBy(\"dest_ip\", \"dest_port\", \"date\", \"hour\", \"private\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_dest_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This part is for sliding window... did not get to compute it yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to set up a sliding window for time bucketed by hour\n",
    "#step 1... sort the dataframe from timestamp by order\n",
    "#step 1b... convert timestamp to unix_timestamp\n",
    "## this was already done above before with netflow_data\n",
    "netflow_data = netflow_data.orderBy(unix_timestamp(netflow_data[\"timestamp\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2... set up a window of time\n",
    "#get the number of seconds in a day\n",
    "#function to calculate number of seconds from number of days: thanks Bob Swain\n",
    "days = lambda i: i * 86400\n",
    "\n",
    "#need timestamp as seconds for our Window time frame\n",
    "#DO NOT NEED THE BELOW CODE LINE... 'timestamp' IS ALREADY A TIMESTAMP\n",
    "test_df = netflow_df.withColumn('timestamp_sec', netflow_df.timestamp.cast('timestamp'))\n",
    "#df.orderBy(org.apache.spark.sql.functions.col(\"columnname\").desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NEW COLUMNS for the date, hour, and second for calculations\n",
    "netflow_data = (netflow_data\n",
    "  .withColumn(\"date\", col(\"timestamp\").cast(\"date\"))\n",
    "  .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "  .withColumn(\"second\", second(col('timestamp'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_dest_day = netflow_data.groupBy(\"dest_ip\", \"dest_port\", \"date\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+-----+\n",
      "|     dest_ip|dest_port|      date|count|\n",
      "+------------+---------+----------+-----+\n",
      "|  10.0.1.100|      135|2017-08-23| 8865|\n",
      "|  10.0.1.100|      135|2017-08-05| 9390|\n",
      "|172.31.4.249|       80|2017-08-18|10424|\n",
      "|172.31.10.10|      443|2017-08-08|15709|\n",
      "|172.31.10.10|      443|2017-08-27|13295|\n",
      "|  10.0.1.100|      135|2017-08-30| 8865|\n",
      "|  10.0.1.100|      135|2017-08-16| 9801|\n",
      "|  10.0.1.100|    49155|2017-08-03| 5027|\n",
      "|  10.0.1.100|    49155|2017-08-08| 5253|\n",
      "|  10.0.1.100|      135|2017-08-10| 9552|\n",
      "|172.31.10.10|      443|2017-08-21|13295|\n",
      "|  10.0.1.100|    49155|2017-08-26| 8504|\n",
      "|172.31.10.10|      443|2017-08-30|13295|\n",
      "|172.31.6.251|       22|2017-08-19| 7580|\n",
      "|172.31.4.249|       80|2017-08-19|10599|\n",
      "|  10.0.1.100|      135|2017-08-25|11112|\n",
      "|172.31.4.249|       80|2017-08-06|10384|\n",
      "|172.31.10.10|      443|2017-08-03|15331|\n",
      "|  10.0.1.100|    49155|2017-08-11| 5152|\n",
      "|  10.0.1.100|      135|2017-08-26|16770|\n",
      "+------------+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggs_dest_day.where('count > 5000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the aggregates\n",
    "aggs_dest_port =netflow_data.groupBy(\"dest_ip\", \"dest_port\", \"date\", \"hour\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+----+-----+\n",
      "|     dest_ip|dest_port|      date|hour|count|\n",
      "+------------+---------+----------+----+-----+\n",
      "|172.31.10.10|      443|2017-08-18|  23| 3891|\n",
      "|  10.0.1.100|      135|2017-08-18|  23| 2623|\n",
      "|172.31.10.10|      443|2017-08-19|   6| 4179|\n",
      "|172.31.10.10|      443|2017-08-26|   8| 3021|\n",
      "|  10.0.1.100|    49155|2017-08-29|  10| 1233|\n",
      "|45.77.65.211|      443|2017-08-25|   4| 2038|\n",
      "|  10.0.1.100|      135|2017-08-19|   7| 2943|\n",
      "|  10.0.1.100|      135|2017-08-24|   3| 2283|\n",
      "|172.31.4.249|       80|2017-08-11|  15| 1003|\n",
      "|172.31.10.10|      443|2017-08-25|   5| 2511|\n",
      "|  10.0.1.100|    49155|2017-08-19|  12| 1579|\n",
      "|172.31.10.10|      443|2017-08-19|  12| 4589|\n",
      "|  10.0.1.100|    49155|2017-08-19|   9| 1642|\n",
      "|  10.0.1.100|      135|2017-08-29|  10| 2477|\n",
      "|  10.0.1.100|      135|2017-08-25|   5| 1484|\n",
      "|  10.0.1.100|    49155|2017-08-26|   7| 1459|\n",
      "|172.31.10.10|      443|2017-08-08|  13| 1306|\n",
      "|  10.0.1.100|    49155|2017-08-19|   4| 1388|\n",
      "|45.77.65.211|      443|2017-08-26|   6| 5471|\n",
      "|172.31.10.10|      443|2017-08-18|  21| 3763|\n",
      "+------------+---------+----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#identify if any of the port counts by date -> hour is greater than 1000\n",
    "aggs_dest_port.where(\"count > 1000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the aggregates\n",
    "aggs_dest_ip = netflow_data.groupBy(\"dest_ip\", \"date\", \"hour\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_dest_ip.where(\"date == '2017-08-19' AND hour == 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----+-----+\n",
      "|      dest_ip|dest_port|      date|hour|count|\n",
      "+-------------+---------+----------+----+-----+\n",
      "| 172.31.6.251|       22|2017-08-19|   3|  725|\n",
      "|52.42.208.228|       22|2017-08-19|   3|    1|\n",
      "| 172.31.4.249|       22|2017-08-19|   3|  215|\n",
      "| 172.31.10.10|       22|2017-08-19|   3|   20|\n",
      "|172.31.38.181|       22|2017-08-19|   3|   15|\n",
      "| 172.31.13.46|       22|2017-08-19|   3|   11|\n",
      "| 172.31.7.219|       22|2017-08-19|   3|   13|\n",
      "+-------------+---------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggs_dest_ip.where(\"date == '2017-08-19' AND hour == 3 AND dest_port == 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label ssh traffic with the given date, hour, and dest_port  \n",
    "netflow_data = netflow_data.select(col(\"*\"), fn.expr(\"case when date == '2017-08-19' AND hour == 3 AND dest_port == 22 then 1 else 0 end\").alias(\"bad_traffic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label suspicious traffic with the given date, hour, and dest_port  \n",
    "netflow_data = netflow_data.select(col(\"*\"), fn.expr(\"case when date == '2017-08-11' AND hour == 14 AND dest_port == 80 OR date == '2017-08-18' AND hour == 22 AND dest_port == 443 OR date == '2017-08-26' AND hour == 6 AND dest_port == 443 OR date == '2017-08-26' AND hour == 7 AND dest_port == 443 then 1 else 0 end\").alias(\"suspicious_traffic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_pd = netflow_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pandas dataframe locally\n",
    "netflow_pd.to_csv('netflow_data.csv', sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
