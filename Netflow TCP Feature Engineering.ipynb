{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Koalas into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: koalas in /opt/conda/lib/python3.6/site-packages (0.21.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from koalas) (3.1.1)\n",
      "Requirement already satisfied: pyarrow<0.15,>=0.10 in /opt/conda/lib/python3.6/site-packages (from koalas) (0.11.1)\n",
      "Requirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.6/site-packages (from koalas) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.6/site-packages (from koalas) (1.15.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.0.0->koalas) (2.7.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.0.0->koalas) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.0.0->koalas) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=3.0.0->koalas) (2.3.0)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from pyarrow<0.15,>=0.10->koalas) (1.12.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.23.2->koalas) (2018.9)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->koalas) (40.6.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install koalas\n",
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this part\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import clustering, regression, evaluation\n",
    "from pyspark.ml import clustering \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark.sql.functions import col, expr, when\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the netflow data\n",
    "netflow = spark.read.csv('netflow.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest_ip',\n",
       " 'dest_port',\n",
       " 'src_ip',\n",
       " 'src_port',\n",
       " 'app',\n",
       " 'bytes',\n",
       " 'bytes_in',\n",
       " 'bytes_out',\n",
       " 'timestamp',\n",
       " 'endtime',\n",
       " '_indextime',\n",
       " '_raw',\n",
       " '_sourcetype',\n",
       " '_subsecond',\n",
       " '_time']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not need the last 3 columns because it is Splunk add-ons\n",
    "netflow_tcp = netflow.select(netflow.columns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of distinct destination IP addresses in the dataset\n",
    "netflow_tcp.select(\"dest_ip\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of distinct dest_ip dest_port combinations\n",
    "netflow_tcp.select(\"dest_ip\", \"dest_port\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of distinct source IP addresses in the dataset\n",
    "netflow_tcp.select(\"src_ip\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of distinct src_ip src_port combinations\n",
    "netflow_tcp.select(\"src_ip\", \"src_port\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets identify how many IP Addresses are in the dest_ip range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_tcp.groupBy('dest_ip').count().show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return shows there is over 200.  Some appear often, othe appear only once.  Will break up by hour and then calculate by hour the distribution of connections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Web Upload & Total Bytes z-score Distribution\n",
    "#### Steps\n",
    "- 1 develop a user defined function (udf)\n",
    "- 2 calcualte mean and standard deviation\n",
    "- 3 run the udf withColumn bytes calcualted\n",
    "- 4 classify all rows where z-score is higher than 2.58 or less than -2.58\n",
    "- 5 classify all rows where total bytes is larger than 35MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col\n",
    "#mean total bytes\n",
    "mean = netflow_tcp.select(mean(col(\"bytes\"))).alias(\"mean\").collect()[0]\n",
    "#standard deviation\n",
    "stdev = netflow_tcp.select(stddev(\"bytes\")).alias(\"sdev\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#compute the z-score for each total bytes\n",
    "def z_score(col):\n",
    "    #average = mean(col)\n",
    "    #standard_dev = stddev(col)\n",
    "    #score = np.absolute((col - mean[0]) / stdev[0])\n",
    "    return (col - mean[0]) / stdev[0]\n",
    "z_udf = udf(z_score)\n",
    "\n",
    "netflow_z = netflow_tcp.withColumn(\"z_score\", z_udf(\"bytes\"))\n",
    "#netflow_z = netflow_z.select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify z-score higher than 2.58 as 1 else 0 => 2.58 is z_score between 3 and 3 standard deviations.  \n",
    "netflow_z = netflow_z.select(col(\"*\"), fn.expr(\"case when z_score > 2.58 OR z_score < -2.58 then 1 else 0 end\").alias(\"z_score_class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------------+--------+-----------+-------------+\n",
      "|       dest_ip|dest_port|      src_ip|src_port|        app|z_score_class|\n",
      "+--------------+---------+------------+--------+-----------+-------------+\n",
      "| 52.42.208.228|     3306|172.31.10.10|   42328|        ssl|            1|\n",
      "|  172.31.4.249|     3306|52.40.10.231|   41479|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   41479|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   37954|        ssl|            1|\n",
      "|199.117.103.25|       80|  10.0.2.101|   56914|        cnn|            1|\n",
      "|199.117.103.33|       80|  10.0.2.101|   56753|        cnn|            1|\n",
      "|173.222.45.139|      443|  10.0.2.101|   56522|     sizmek|            1|\n",
      "|173.194.54.218|       80|  10.0.2.101|   56041|google_play|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   42328|        ssl|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   41479|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   37954|        ssl|            1|\n",
      "|199.117.103.33|       80|  10.0.2.101|   56753|        cnn|            1|\n",
      "|173.222.45.139|      443|  10.0.2.101|   56522|     sizmek|            1|\n",
      "|173.194.54.218|       80|  10.0.2.107|   62366|google_play|            1|\n",
      "|173.194.54.218|       80|  10.0.2.101|   56041|google_play|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   43182|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   43178|      mysql|            1|\n",
      "|173.222.45.139|      443|  10.0.2.101|   56522|     sizmek|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   43178|      mysql|            1|\n",
      "| 52.42.208.228|     3306|172.31.10.10|   37954|        ssl|            1|\n",
      "+--------------+---------+------------+--------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check to see which dest_port and src_ip combinations have a z_score_class of 1\n",
    "netflow_z.select(\"dest_ip\", \"dest_port\", \"src_ip\", \"src_port\", \"app\", \"z_score_class\").where(\"z_score_class == 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns 5198 values where the z_score is greater than 2.58... or 99% confidence interval or greater. Additionally, looking at the traffic above, the IP address 52.42.208.228 is the **destination** and is using port 3306, which is standard for MYSQL with SSL or other encryption  \n",
    "\n",
    "App sizmek is an amazon company which is used for advertising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to understand the ports and IP addresses with z_score_class of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----+\n",
      "|       dest_ip|dest_port|count|\n",
      "+--------------+---------+-----+\n",
      "|    10.0.2.107|    59100|    1|\n",
      "|    10.0.2.107|    58729|    1|\n",
      "|173.194.54.218|       80|  749|\n",
      "|    10.0.2.107|    59360|    1|\n",
      "|    10.0.2.107|    58336|    1|\n",
      "|    10.0.2.107|    58912|    1|\n",
      "|  172.31.10.10|    10001|    7|\n",
      "|  78.47.38.226|      443|    1|\n",
      "|    10.0.2.107|    58503|    1|\n",
      "|    10.0.2.107|    58763|    1|\n",
      "|    10.0.2.107|    59279|    1|\n",
      "|  172.31.10.10|      443|   13|\n",
      "|    10.0.2.107|    59352|    1|\n",
      "|    10.0.2.107|    57403|    1|\n",
      "|    10.0.2.107|    58900|    1|\n",
      "|    10.0.2.107|    59273|    1|\n",
      "|    10.0.2.107|    59140|    1|\n",
      "|    10.0.2.107|    59132|    1|\n",
      "|    10.0.2.107|    58457|    1|\n",
      "|    10.0.2.107|    57196|    1|\n",
      "+--------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_z.select(\"dest_ip\", \"dest_port\", \"src_ip\", \"src_port\", \"app\", \"z_score_class\").where(\"z_score_class == 1\").groupBy(\"dest_ip\",\"dest_port\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The port grouping shows IP address 172.31.10.10 connecting on 10001 and 443.  Port 10001 is used by protocol SCP and used for online gaming, trojans, backdoors, and Network Attached Storage (NAS).  The IP Address is in the private range and not routeable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       dest_ip|count|\n",
      "+--------------+-----+\n",
      "| 50.19.160.254|    1|\n",
      "|    10.0.1.200|    1|\n",
      "|  172.31.10.10|   23|\n",
      "|199.117.103.33|  372|\n",
      "|   40.97.51.66|    1|\n",
      "|    10.0.2.107|   51|\n",
      "|  78.47.38.226|    1|\n",
      "|173.194.54.218|  749|\n",
      "|  52.40.10.231|  160|\n",
      "|    10.0.2.109|    1|\n",
      "|199.117.103.25|  279|\n",
      "|    10.0.1.101|    3|\n",
      "| 67.132.183.74|    1|\n",
      "|  172.31.4.249|  658|\n",
      "| 52.42.208.228| 2400|\n",
      "|173.222.45.139|  496|\n",
      "| 151.101.0.223|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_z.select(\"dest_ip\", \"dest_port\", \"src_ip\", \"src_port\", \"app\", \"z_score_class\").where(\"z_score_class == 1\").groupBy(\"dest_ip\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the distribution of IP addresses that have anomolous bytes size based on z_score > 2.58 OR < -2.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Web Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8ec8dcf1e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the number of records where the value is greater than 35MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetflow_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bytes > 35000000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns 78 counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# get the number of records where the value is greater than 35MB\n",
    "netflow_z.where('bytes > 35000000').count().show() # returns 78 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label all the traffic with large web upload.\n",
    "netflow_z_up = netflow_z.select(\"*\", when(col(\"bytes\") > 35000000, 1).otherwise(0).alias(\"large_upload\"))\n",
    "\n",
    "#test = netflow_data.select(col('*'), fn.expr(\"case when bytes > 35000000 AND dest_ip != '52.42.208.228' then 1 else 0 end\").alias(\"largest_upload\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where('large_upload = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.where('largest_upload = 1').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label TOR traffic\n",
    "#### Steps\n",
    "Tor typically uses TCP ports 9001, 9030, 9150\n",
    "- 1 find all dest and src ports where ports match with TOR traffic\n",
    "- 2 label the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_spam2_df = sms_spam_df.select('text', when(col(\"type\") == 'spam', 1.0).otherwise(0).alias('type'))\n",
    "netflow_z.where('dest_port == 9001 OR dest_port == 9030 OR dest_port == 9150 OR dest_port == 31337').count() #returns 28 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_z.where('src_port == 9001 OR src_port == 9030 OR src_port == 9150 OR src_port == 31337').count() #returns 8 counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column for all flows that meet the case of meeting one or more of the tor port conditions... where if yes then 1, otherwise 0.\n",
    "\n",
    "netflow_z_up = netflow_z_up.select(col(\"*\"), fn.expr(\"case when dest_port == 9001 OR dest_port == 9030 OR dest_port == 9150 OR dest_port == 31337 OR src_port == 9001 OR src_port == 9030 OR src_port == 9150 OR src_port == 31337 then 1 else 0 end\").alias(\"tor_class\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Difference error Distribution\n",
    "#### Steps\n",
    "- 1 create a new column with the diff of timestamp and endtime\n",
    "- 2 get the mean and standard deviation of the time\n",
    "- 3 calculate the z-score for the time\n",
    "- 4 label all times that are outliers (+- 2.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second, mean, unix_timestamp, substring, abs\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as func\n",
    "from scipy import stats\n",
    "from datetime import timedelta\n",
    "#newDF = df.orderBy(unix_timestamp(df(\"stringCol\"), pattern).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the dataframe by 'timestamp'\n",
    "netflow_z = netflow_z_up.orderBy(unix_timestamp(netflow_z[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timestamp', 'timestamp'),\n",
       " ('endtime', 'timestamp'),\n",
       " ('z_score', 'string'),\n",
       " ('z_score_class', 'int'),\n",
       " ('large_upload', 'int'),\n",
       " ('tor_class', 'int')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_z.dtypes[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Diff\n",
    "timeFmt = \"yyyy-MM-dd' 'HH:mm:ss.SSS\"\n",
    "#netflow_time_diff = netflow_z_up\\\n",
    "netflow_time_diff = netflow_z\\\n",
    "    .withColumn('start_msec', unix_timestamp('timestamp', format=timeFmt) + substring('timestamp', -3, 3).cast('float')/1000)\\\n",
    "    .withColumn('end_msec', unix_timestamp('endtime', format=timeFmt) + substring('endtime', -3, 3).cast('float')/1000)\\\n",
    "    .withColumn('diff_msec', (abs(col('start_msec') - col('end_msec')).cast('double')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col\n",
    "\n",
    "#get the mean of diff time\n",
    "#mean total bytes\n",
    "mean_msec = netflow_time_diff.select(mean(col(\"diff_msec\"))).alias(\"mean_msec\").collect()[0]\n",
    "#get the standard deviation of diff time\n",
    "#standard deviation\n",
    "stdev_msec = netflow_time_diff.select(stddev(\"diff_msec\")).alias(\"stdev_msec\").collect()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 299.23850464502044 standard dev: 613.9616176450414\n"
     ]
    }
   ],
   "source": [
    "print(\"mean:\", mean_msec[0], \"standard dev:\", stdev_msec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#create a user defined function for the diff time\n",
    "def z_score_time(col):\n",
    "    return (col - mean_msec[0]) / stdev_msec[0]\n",
    "z_udf = udf(lambda z: z_score_time(z), DoubleType())\n",
    "\n",
    "netflow_time = netflow_time_diff.select('*', z_udf(\"diff_msec\").alias('z_score_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|z_score_time        |\n",
      "+--------------------+\n",
      "|60.36249951593672   |\n",
      "|60.36249951593672   |\n",
      "|60.36249951593672   |\n",
      "|60.36249951593672   |\n",
      "|-0.21945590481709806|\n",
      "|0.31720629813789736 |\n",
      "|-0.2111035916665231 |\n",
      "|-0.38229995792985855|\n",
      "|-0.47661529859911056|\n",
      "|-0.09770236778612007|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_time.select(\"z_score_time\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o360.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 816, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ed5c86c25cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetflow_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z_score_time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z_score_time > 2.58\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o360.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 816, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-580685a6bf38>\", line 10, in <lambda>\n  File \"<ipython-input-28-580685a6bf38>\", line 9, in z_score_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#####ERRRRROOOORRRRR ######\n",
    "#this does not work because ... well... I just cannot figure out why it does not work. Something to do with unsupported type\n",
    "#TypeError: unsupported operand type(s) for -: 'NoneType' and 'float' -> referencing z_score_time\n",
    "\n",
    "netflow_time.select('z_score_time').where(\"z_score_time > 2.58\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2036985"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull\n",
    "#df.where(df.a.isNull()).count()\n",
    "netflow_time_diff.select(isnull('time_z_score')).count()\n",
    "\n",
    "\n",
    "#.where(netflow_time_diff.z_score_time.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another techinque to cast the time as a Double type\n",
    "from pyspark.sql.types import DoubleType\n",
    "netflow_time_diff = netflow_time_diff.withColumn(\"time_z_score\", netflow_time_diff[\"z_score_time\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        time_z_score|\n",
      "+--------------------+\n",
      "|   60.36249909441595|\n",
      "|   60.36249909441595|\n",
      "|   60.36249909441595|\n",
      "|   60.36249909441595|\n",
      "|-0.21945589706690755|\n",
      "|  0.3172062993391025|\n",
      "| -0.2111035985269344|\n",
      "| -0.3822999584602314|\n",
      "| -0.4766153069898921|\n",
      "|  -0.097702359462893|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_time_diff.select(\"time_z_score\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify z-score higher than 2.58 as 1 else 0 => 2.58 is z_score between 3 and 3 standard deviations.  \n",
    "netflow_df = netflow_time_diff.select(col(\"*\"), fn.expr(\"case when z_score_time > 2 OR z_score_time < -2 then 1 else 0 end\").alias(\"z_score_time_class\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New IP Address connection from private subnet\n",
    "#### Steps\n",
    "- 1 Identify Private IP addresses\n",
    "- 2 Calculate IP addresses per hour\n",
    "- 3 If IP address per hour is new, label it anomolous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order the dataframe by 'timestamp'\n",
    "netflow_z = netflow_time_diff.orderBy(unix_timestamp(netflow_z[\"timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_pd = netflow_z.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>app</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>endtime</th>\n",
       "      <th>z_score</th>\n",
       "      <th>diff_time_ms</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0.1.100</td>\n",
       "      <td>135</td>\n",
       "      <td>10.0.1.1</td>\n",
       "      <td>39814</td>\n",
       "      <td>msrpc</td>\n",
       "      <td>1470</td>\n",
       "      <td>838</td>\n",
       "      <td>632</td>\n",
       "      <td>2017-08-01 18:29:25.404</td>\n",
       "      <td>2017-08-01 18:29:35.335</td>\n",
       "      <td>-0.024271898960842864</td>\n",
       "      <td>00:00:09.931000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.31.10.10</td>\n",
       "      <td>443</td>\n",
       "      <td>52.34.175.192</td>\n",
       "      <td>60761</td>\n",
       "      <td>ssl</td>\n",
       "      <td>5045</td>\n",
       "      <td>1926</td>\n",
       "      <td>3119</td>\n",
       "      <td>2017-08-01 18:31:09.521</td>\n",
       "      <td>2017-08-01 18:22:16.052</td>\n",
       "      <td>-0.023062881596103615</td>\n",
       "      <td>00:08:53.469000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dest_ip  dest_port         src_ip  src_port    app  bytes  bytes_in  \\\n",
       "0    10.0.1.100        135       10.0.1.1     39814  msrpc   1470       838   \n",
       "1  172.31.10.10        443  52.34.175.192     60761    ssl   5045      1926   \n",
       "\n",
       "   bytes_out               timestamp                 endtime  \\\n",
       "0        632 2017-08-01 18:29:25.404 2017-08-01 18:29:35.335   \n",
       "1       3119 2017-08-01 18:31:09.521 2017-08-01 18:22:16.052   \n",
       "\n",
       "                 z_score    diff_time_ms private  \n",
       "0  -0.024271898960842864 00:00:09.931000       0  \n",
       "1  -0.023062881596103615 00:08:53.469000     NaN  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through dataframe... if private to private match, the return 0, else will return NaN \n",
    "netflow_pd.loc[(netflow_pd.dest_ip.str.contains('10.0|172.31|192.168')) & (netflow_pd.src_ip.str.contains('10.0|172.31|192.168')), 'private'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all values of NaN with 1 meaning 1 is not a private to private mapping\n",
    "netflow_pd['private'] = netflow_pd.private.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>app</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>endtime</th>\n",
       "      <th>z_score</th>\n",
       "      <th>z_score_class</th>\n",
       "      <th>large_upload</th>\n",
       "      <th>tor_class</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.97.128.34</td>\n",
       "      <td>443</td>\n",
       "      <td>10.0.2.109</td>\n",
       "      <td>58107</td>\n",
       "      <td>office365</td>\n",
       "      <td>1441608</td>\n",
       "      <td>167497</td>\n",
       "      <td>1274111</td>\n",
       "      <td>2017-07-31 14:46:43.264</td>\n",
       "      <td>2017-08-01 01:09:22.761</td>\n",
       "      <td>0.46276358345058344</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dest_ip  dest_port      src_ip  src_port        app    bytes  \\\n",
       "0  40.97.128.34        443  10.0.2.109     58107  office365  1441608   \n",
       "\n",
       "   bytes_in  bytes_out               timestamp                 endtime  \\\n",
       "0    167497    1274111 2017-07-31 14:46:43.264 2017-08-01 01:09:22.761   \n",
       "\n",
       "               z_score  z_score_class  large_upload  tor_class  private  \n",
       "0  0.46276358345058344              0             0          0      1.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if the non_private \n",
    "netflow_pd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pandas dataframe locally\n",
    "netflow_pd.to_csv('netflow_data.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9849bc6400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create historgram showing count of private to private and not\n",
    "netflow_pd['private'].value_counts().plot(kind='bar')\n",
    "#we can see there is a significant amount of non private to private communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataframe back to a spark, this will contain a new data_frame that has a private column \n",
    "#netflow_data = spark.createDataFrame(netflow_pd)\n",
    "netflow_data = spark.read.csv('netflow_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This part is for sliding window... did not get to compute it yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to set up a sliding window for time bucketed by hour\n",
    "#step 1... sort the dataframe from timestamp by order\n",
    "#step 1b... convert timestamp to unix_timestamp\n",
    "## this was already done above before with netflow_data\n",
    "netflow_data = netflow_data.orderBy(unix_timestamp(netflow_data[\"timestamp\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2... set up a window of time\n",
    "#get the number of seconds in a day\n",
    "#function to calculate number of seconds from number of days: thanks Bob Swain\n",
    "days = lambda i: i * 86400\n",
    "\n",
    "#need timestamp as seconds for our Window time frame\n",
    "#DO NOT NEED THE BELOW CODE LINE... 'timestamp' IS ALREADY A TIMESTAMP\n",
    "test_df = netflow_df.withColumn('timestamp_sec', netflow_df.timestamp.cast('timestamp'))\n",
    "#df.orderBy(org.apache.spark.sql.functions.col(\"columnname\").desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NEW COLUMNS for the date, hour, and second for calculations\n",
    "netflow_data = (netflow_data\n",
    "  .withColumn(\"date\", col(\"timestamp\").cast(\"date\"))\n",
    "  .withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "  .withColumn(\"second\", second(col('timestamp'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the counts \n",
    "aggs_dest_test = netflow_data.groupBy(\"dest_ip\", \"dest_port\", \"date\", \"hour\", \"private\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----------+----+-------+-----+\n",
      "|        dest_ip|dest_port|      date|hour|private|count|\n",
      "+---------------+---------+----------+----+-------+-----+\n",
      "|  67.132.183.11|       80|2017-08-01|   1|    1.0|    1|\n",
      "|   40.97.30.162|      443|2017-08-01|   2|    1.0|    1|\n",
      "| 46.137.111.128|       80|2017-08-01|   5|    1.0|    1|\n",
      "|     8.43.72.32|       80|2017-08-01|   6|    1.0|    1|\n",
      "| 34.205.236.225|       80|2017-08-01|   8|    1.0|    1|\n",
      "|   198.8.70.211|       80|2017-08-01|   9|    1.0|    2|\n",
      "|  88.198.33.189|      443|2017-08-01|  11|    1.0|    2|\n",
      "|173.241.244.143|       80|2017-08-01|  12|    1.0|    2|\n",
      "|  104.45.11.195|      443|2017-08-01|  14|    1.0|    1|\n",
      "| 149.174.66.131|       80|2017-08-01|  15|    1.0|    1|\n",
      "|  68.67.178.132|       80|2017-08-01|  15|    1.0|    1|\n",
      "|   74.125.161.6|       80|2017-08-01|  15|    1.0|    1|\n",
      "|  66.235.153.37|       80|2017-08-01|  17|    1.0|    4|\n",
      "|   23.40.20.181|      443|2017-08-01|  17|    1.0|    1|\n",
      "| 151.80.231.118|       80|2017-08-01|  17|    1.0|    1|\n",
      "|  54.241.31.242|       80|2017-08-01|  19|    1.0|    1|\n",
      "|  52.42.208.228|       80|2017-08-01|  19|    1.0|    9|\n",
      "| 192.230.66.187|       80|2017-08-01|  19|    1.0|    1|\n",
      "| 172.217.11.230|      443|2017-08-01|  21|    1.0|    2|\n",
      "|  18.220.77.203|       80|2017-08-01|  21|    1.0|    1|\n",
      "+---------------+---------+----------+----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggs_dest_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_dest_day = netflow_data.groupBy(\"dest_ip\", \"dest_port\", \"date\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+-----+\n",
      "|     dest_ip|dest_port|      date|count|\n",
      "+------------+---------+----------+-----+\n",
      "|  10.0.1.100|      135|2017-08-23| 8865|\n",
      "|  10.0.1.100|      135|2017-08-05| 9390|\n",
      "|172.31.4.249|       80|2017-08-18|10424|\n",
      "|172.31.10.10|      443|2017-08-08|15709|\n",
      "|172.31.10.10|      443|2017-08-27|13295|\n",
      "|  10.0.1.100|      135|2017-08-30| 8865|\n",
      "|  10.0.1.100|      135|2017-08-16| 9801|\n",
      "|  10.0.1.100|    49155|2017-08-03| 5027|\n",
      "|  10.0.1.100|    49155|2017-08-08| 5253|\n",
      "|  10.0.1.100|      135|2017-08-10| 9552|\n",
      "|172.31.10.10|      443|2017-08-21|13295|\n",
      "|  10.0.1.100|    49155|2017-08-26| 8504|\n",
      "|172.31.10.10|      443|2017-08-30|13295|\n",
      "|172.31.6.251|       22|2017-08-19| 7580|\n",
      "|172.31.4.249|       80|2017-08-19|10599|\n",
      "|  10.0.1.100|      135|2017-08-25|11112|\n",
      "|172.31.4.249|       80|2017-08-06|10384|\n",
      "|172.31.10.10|      443|2017-08-03|15331|\n",
      "|  10.0.1.100|    49155|2017-08-11| 5152|\n",
      "|  10.0.1.100|      135|2017-08-26|16770|\n",
      "+------------+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggs_dest_day.where('count > 5000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the aggregates\n",
    "aggs_dest_port =netflow_data.groupBy(\"dest_ip\", \"dest_port\", \"date\", \"hour\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+----+-----+\n",
      "|     dest_ip|dest_port|      date|hour|count|\n",
      "+------------+---------+----------+----+-----+\n",
      "|172.31.10.10|      443|2017-08-18|  23| 3891|\n",
      "|  10.0.1.100|      135|2017-08-18|  23| 2623|\n",
      "|172.31.10.10|      443|2017-08-19|   6| 4179|\n",
      "|172.31.10.10|      443|2017-08-26|   8| 3021|\n",
      "|  10.0.1.100|    49155|2017-08-29|  10| 1233|\n",
      "|45.77.65.211|      443|2017-08-25|   4| 2038|\n",
      "|  10.0.1.100|      135|2017-08-19|   7| 2943|\n",
      "|  10.0.1.100|      135|2017-08-24|   3| 2283|\n",
      "|172.31.4.249|       80|2017-08-11|  15| 1003|\n",
      "|172.31.10.10|      443|2017-08-25|   5| 2511|\n",
      "|  10.0.1.100|    49155|2017-08-19|  12| 1579|\n",
      "|172.31.10.10|      443|2017-08-19|  12| 4589|\n",
      "|  10.0.1.100|    49155|2017-08-19|   9| 1642|\n",
      "|  10.0.1.100|      135|2017-08-29|  10| 2477|\n",
      "|  10.0.1.100|      135|2017-08-25|   5| 1484|\n",
      "|  10.0.1.100|    49155|2017-08-26|   7| 1459|\n",
      "|172.31.10.10|      443|2017-08-08|  13| 1306|\n",
      "|  10.0.1.100|    49155|2017-08-19|   4| 1388|\n",
      "|45.77.65.211|      443|2017-08-26|   6| 5471|\n",
      "|172.31.10.10|      443|2017-08-18|  21| 3763|\n",
      "+------------+---------+----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#identify if any of the port counts by date -> hour is greater than 1000\n",
    "aggs_dest_port.where(\"count > 1000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the aggregates\n",
    "aggs_dest_ip = netflow_data.groupBy(\"dest_ip\", \"date\", \"hour\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----+-----+\n",
      "|        dest_ip|      date|hour|count|\n",
      "+---------------+----------+----+-----+\n",
      "|   40.97.85.114|2017-08-19|   3|    1|\n",
      "|  168.61.146.25|2017-08-19|   3|    2|\n",
      "| 95.154.251.133|2017-08-19|   3|    1|\n",
      "|    23.1.26.102|2017-08-19|   3|    1|\n",
      "|   40.97.142.34|2017-08-19|   3|    7|\n",
      "|   172.31.13.46|2017-08-19|   3|   11|\n",
      "|   13.107.9.152|2017-08-19|   3|    1|\n",
      "|  40.97.125.114|2017-08-19|   3|    1|\n",
      "|   40.113.10.78|2017-08-19|   3|    3|\n",
      "| 40.121.144.182|2017-08-19|   3|    1|\n",
      "|  54.69.178.232|2017-08-19|   3|    1|\n",
      "|  68.67.178.243|2017-08-19|   3|    1|\n",
      "|   23.100.86.91|2017-08-19|   3|    3|\n",
      "|  40.97.113.162|2017-08-19|   3|    1|\n",
      "|  40.97.113.178|2017-08-19|   3|    1|\n",
      "|    69.169.85.7|2017-08-19|   3|    1|\n",
      "| 23.208.218.213|2017-08-19|   3|    1|\n",
      "|151.101.128.249|2017-08-19|   3|    1|\n",
      "|     10.0.2.108|2017-08-19|   3|   13|\n",
      "| 23.208.216.187|2017-08-19|   3|    1|\n",
      "+---------------+----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggs_dest_ip.where(\"date == '2017-08-19' AND hour == 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`dest_port`' given input columns: [dest_ip, date, hour, count]; line 1 pos 39;\\n'Filter (((cast(date#364 as string) = 2017-08-19) && (hour#382 = 3)) && ('dest_port = 22))\\n+- Aggregate [dest_ip#297, date#364, hour#382], [dest_ip#297, date#364, hour#382, count(1) AS count#589L]\\n   +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, date#364, hour#382, second(timestamp#305, Some(GMT)) AS second#401]\\n      +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, date#364, hour(timestamp#305, Some(GMT)) AS hour#382]\\n         +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, cast(timestamp#305 as date) AS date#364]\\n            +- Sort [unix_timestamp(timestamp#305, yyyy-MM-dd HH:mm:ss, Some(GMT)) ASC NULLS FIRST], true\\n               +- Relation[_c0#296,dest_ip#297,dest_port#298,src_ip#299,src_port#300,app#301,bytes#302L,bytes_in#303,bytes_out#304,timestamp#305,endtime#306,z_score#307,z_score_class#308,large_upload#309,tor_class#310,private#311] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o231.filter.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`dest_port`' given input columns: [dest_ip, date, hour, count]; line 1 pos 39;\n'Filter (((cast(date#364 as string) = 2017-08-19) && (hour#382 = 3)) && ('dest_port = 22))\n+- Aggregate [dest_ip#297, date#364, hour#382], [dest_ip#297, date#364, hour#382, count(1) AS count#589L]\n   +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, date#364, hour#382, second(timestamp#305, Some(GMT)) AS second#401]\n      +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, date#364, hour(timestamp#305, Some(GMT)) AS hour#382]\n         +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, cast(timestamp#305 as date) AS date#364]\n            +- Sort [unix_timestamp(timestamp#305, yyyy-MM-dd HH:mm:ss, Some(GMT)) ASC NULLS FIRST], true\n               +- Relation[_c0#296,dest_ip#297,dest_port#298,src_ip#299,src_port#300,app#301,bytes#302L,bytes_in#303,bytes_out#304,timestamp#305,endtime#306,z_score#307,z_score_class#308,large_upload#309,tor_class#310,private#311] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:177)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:65)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1485)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1499)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3a4ba530b67f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maggs_dest_ip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date == '2017-08-19' AND hour == 3 AND dest_port == 22\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m   1357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`dest_port`' given input columns: [dest_ip, date, hour, count]; line 1 pos 39;\\n'Filter (((cast(date#364 as string) = 2017-08-19) && (hour#382 = 3)) && ('dest_port = 22))\\n+- Aggregate [dest_ip#297, date#364, hour#382], [dest_ip#297, date#364, hour#382, count(1) AS count#589L]\\n   +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, date#364, hour#382, second(timestamp#305, Some(GMT)) AS second#401]\\n      +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, date#364, hour(timestamp#305, Some(GMT)) AS hour#382]\\n         +- Project [_c0#296, dest_ip#297, dest_port#298, src_ip#299, src_port#300, app#301, bytes#302L, bytes_in#303, bytes_out#304, timestamp#305, endtime#306, z_score#307, z_score_class#308, large_upload#309, tor_class#310, private#311, cast(timestamp#305 as date) AS date#364]\\n            +- Sort [unix_timestamp(timestamp#305, yyyy-MM-dd HH:mm:ss, Some(GMT)) ASC NULLS FIRST], true\\n               +- Relation[_c0#296,dest_ip#297,dest_port#298,src_ip#299,src_port#300,app#301,bytes#302L,bytes_in#303,bytes_out#304,timestamp#305,endtime#306,z_score#307,z_score_class#308,large_upload#309,tor_class#310,private#311] csv\\n\""
     ]
    }
   ],
   "source": [
    "aggs_dest_ip.where(\"date == '2017-08-19' AND hour == 3 AND dest_port == 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label ssh traffic with the given date, hour, and dest_port  \n",
    "netflow_data = netflow_data.select(col(\"*\"), fn.expr(\"case when date == '2017-08-19' AND hour == 3 AND dest_port == 22 then 1 else 0 end\").alias(\"bad_traffic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label suspicious traffic with the given date, hour, and dest_port  \n",
    "netflow_data = netflow_data.select(col(\"*\"), fn.expr(\"case when date == '2017-08-11' AND hour == 14 AND dest_port == 80 OR date == '2017-08-18' AND hour == 22 AND dest_port == 443 OR date == '2017-08-26' AND hour == 6 AND dest_port == 443 OR date == '2017-08-26' AND hour == 7 AND dest_port == 443 then 1 else 0 end\").alias(\"suspicious_traffic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the data with Splunk answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the flow data\n",
    "netflow_data = spark.read.csv('netflow_cluster.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'dest_ip', 'dest_port', 'src_ip', 'src_port', 'app', 'bytes', 'bytes_in', 'bytes_out', 'timestamp', 'endtime', 'z_score', 'z_score_class', 'large_upload', 'tor_class', 'private', 'date', 'hour', 'second', 'bad_traffic', 'suspicious_traffic', 'kmeans_feat']\n"
     ]
    }
   ],
   "source": [
    "print(netflow_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_data = netflow_data.drop('z_score_class', 'tor_class', 'suspicious_traffic', 'kmeans_feat', '_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_data = netflow_data.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dest_ip', 'dest_port', 'src_ip', 'src_port', 'app', 'bytes', 'bytes_in', 'bytes_out', 'timestamp', 'endtime', 'z_score', 'large_upload', 'private', 'date', 'hour', 'second', 'bad_traffic', 'bad_ports', 'z_score_alpha', 'bad_ip']\n"
     ]
    }
   ],
   "source": [
    "print(netflow_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|        src_ip|dest_port|\n",
      "+--------------+---------+\n",
      "|    42.7.26.15|       22|\n",
      "| 116.31.116.52|       22|\n",
      "|  185.21.34.80|       22|\n",
      "| 116.31.116.52|       22|\n",
      "| 116.31.116.17|       22|\n",
      "|  58.242.83.11|       22|\n",
      "| 116.31.116.17|       22|\n",
      "|    42.7.26.15|       22|\n",
      "|  58.242.83.11|       22|\n",
      "|31.162.224.135|       22|\n",
      "| 116.31.116.52|       22|\n",
      "|31.162.224.135|       22|\n",
      "|    42.7.26.15|       22|\n",
      "| 116.31.116.17|       22|\n",
      "|221.226.117.24|       22|\n",
      "| 116.31.116.17|       22|\n",
      "| 116.31.116.17|       22|\n",
      "|  71.39.18.121|       22|\n",
      "| 116.31.116.17|       22|\n",
      "| 116.31.116.52|       22|\n",
      "+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_data.select(\"src_ip\", 'dest_port').where('bad_traffic == 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers from Splunk Botsv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "45.77.65.211 = bad\n",
    "x 6881-6889 bittorrent\n",
    "x 10001 online gaming\n",
    "160.153.91.7 = bad\n",
    "104.238.159.19 = bad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tor ports and bittorrent ports\n",
    "netflow_data = netflow_data.select(col(\"*\"), fn.expr(\"case when dest_port == 6881 OR dest_port == 6882 OR dest_port == 6883 OR dest_port == 6884 OR dest_port == 6885 OR dest_port == 6886 OR dest_port == 6887 OR dest_port == 6888 OR dest_port == 6889 OR dest_port == 9001 OR dest_port == 9030 OR dest_port == 9150 OR dest_port == 31337 OR src_port == 9001 OR src_port == 9030 OR src_port == 9150 OR src_port == 10001 then 1 else 0 end\").alias(\"bad_ports\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ip address 52.42.208.228 is the transactional mysql server for the company... remove it from z_score \n",
    "netflow_data = netflow_data.select(col('*'),fn.expr(\"case when dest_ip!= '52.42.208.228' AND z_score > 4 OR dest_ip!= '52.42.208.228' AND z_score < -4 OR src_ip!= '52.42.208.228' AND z_score > 4 OR src_ip!= '52.42.208.228' AND z_score < -4 then 1 else 0 end\").alias(\"z_score_alpha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set bad ip addresses to 1 and all other 0\n",
    "netflow_data = netflow_data.select(col('*'),fn.expr(\"case when dest_ip = '45.77.65.211' OR src_ip = '45.77.65.211' OR dest_ip = '160.153.91.7' OR src_ip = '160.153.91.7' OR dest_ip = '104.238.159.19' OR src_ip = '104.238.159.19' then 1 else 0 end\").alias(\"bad_ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10908"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where('dest_ip = \"52.42.208.228\"').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_data = netflow_data.drop('bad_ip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'dest_ip',\n",
       " 'dest_port',\n",
       " 'src_ip',\n",
       " 'src_port',\n",
       " 'app',\n",
       " 'bytes',\n",
       " 'bytes_in',\n",
       " 'bytes_out',\n",
       " 'timestamp',\n",
       " 'endtime',\n",
       " 'z_score',\n",
       " 'large_upload',\n",
       " 'private',\n",
       " 'date',\n",
       " 'hour',\n",
       " 'second',\n",
       " 'bad_traffic',\n",
       " 'bad_ports',\n",
       " 'z_score_alpha',\n",
       " 'bad_ip']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where('bad_ports = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where(\"bad_ip = 1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where('large_upload = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3452"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where('z_score_alpha = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_data.where('bad_traffic == 1').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_labeled = netflow_data.select(col(\"*\"), fn.expr(\"case when z_score_alpha == 1 OR large_upload == 1 OR bad_ports = 1 OR bad_ip = 1 OR bad_traffic = 1 then 1 else 0 end\").alias(\"anomalous\"))\n",
    "netflow_labeled.where('anomalous == 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_labels = netflow_labeled.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_labels.to_csv('netflow_final_data.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_labeled.toPandas().to_csv('netflow_final.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_pd = netflow_data.to_koalas().to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pandas dataframe locally\n",
    "netflow_pd.to_csv('netflow_data.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label new netflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the netflow data\n",
    "netflow_v1 = spark.read.csv('botsv1_netflow.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[endtime: string, timestamp: string, bytes: int, src_ip: string, src_port: int, bytes_in: int, dest_ip: string, dest_port: int, bytes_out: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tor ports and bittorrent ports\n",
    "netflow_v1 = netflow_v1.select(col(\"*\"), fn.expr(\"case when dest_port == 6881 OR dest_port == 6882 OR dest_port == 6883 OR dest_port == 6884 OR dest_port == 6885 OR dest_port == 6886 OR dest_port == 6887 OR dest_port == 6888 OR dest_port == 6889 OR dest_port == 9001 OR dest_port == 9030 OR dest_port == 9150 OR dest_port == 31337 OR src_port == 9001 OR src_port == 9030 OR src_port == 9150 OR src_port == 10001 then 1 else 0 end\").alias(\"bad_ports\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.where('bad_ports = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label all the traffic with large web upload.\n",
    "netflow_v1 = netflow_v1.select(\"*\", when(col(\"bytes\") > 35000000, 1).otherwise(0).alias(\"large_upload\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.where('large_upload = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col\n",
    "#mean total bytes\n",
    "mean = netflow_v1.select(mean(col(\"bytes\"))).alias(\"mean\").collect()[0]\n",
    "#standard deviation\n",
    "stdev = netflow_v1.select(stddev(\"bytes\")).alias(\"sdev\").collect()[0]\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "#compute the z-score for each total bytes\n",
    "def z_score(col):\n",
    "    #average = mean(col)\n",
    "    #standard_dev = stddev(col)\n",
    "    #score = np.absolute((col - mean[0]) / stdev[0])\n",
    "    return (col - mean[0]) / stdev[0]\n",
    "z_udf = udf(z_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1 = netflow_v1.withColumn(\"z_score\", z_udf(\"bytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1 = netflow_v1.drop('z_score_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'endtime',\n",
       " 'timestamp',\n",
       " 'bytes',\n",
       " 'src_ip',\n",
       " 'src_port',\n",
       " 'bytes_in',\n",
       " 'dest_ip',\n",
       " 'dest_port',\n",
       " 'bytes_out',\n",
       " 'bad_ports',\n",
       " 'large_upload',\n",
       " 'z_score']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify z-score higher than 2.58 as 1 else 0 => 2.58 is z_score between 3 and 3 standard deviations.  \n",
    "netflow_v1 = netflow_v1.select(col(\"*\"), fn.expr(\"case when z_score > 4 OR z_score < -4 then 1 else 0 end\").alias(\"z_score_alpha\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.where('z_score_alpha = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------------+--------+-----+\n",
      "|        dest_ip|dest_port|         src_ip|src_port|bytes|\n",
      "+---------------+---------+---------------+--------+-----+\n",
      "| 192.168.228.67|      445| 192.168.224.68|    2945|39766|\n",
      "|  192.168.231.6|      445|192.168.224.241|   54304| 7967|\n",
      "|192.168.229.204|      445|192.168.227.108|   14005| 7967|\n",
      "| 192.168.224.57|      445| 192.168.231.61|    3233| 7965|\n",
      "|192.168.226.116|      445|192.168.229.147|   28320| 7847|\n",
      "|192.168.229.224|      445|192.168.229.175|   59313| 7967|\n",
      "| 192.168.224.50|      445| 192.168.228.94|   57281|45316|\n",
      "|192.168.225.198|      445| 192.168.227.13|   40758| 7967|\n",
      "|192.168.226.230|      445| 192.168.224.78|    3698| 7965|\n",
      "|192.168.228.100|      445| 192.168.229.49|   21704| 7965|\n",
      "| 192.168.229.78|      445|192.168.227.126|   35152| 7847|\n",
      "|192.168.229.151|      445|192.168.225.244|   61559| 7967|\n",
      "| 192.168.225.20|      445|192.168.230.173|   50739| 7967|\n",
      "|192.168.228.124|      445|192.168.225.181|   59574| 8737|\n",
      "|192.168.228.127|      445|192.168.227.251|   55622| 8737|\n",
      "| 192.168.227.47|      445| 192.168.226.95|    4973| 7965|\n",
      "|192.168.224.209|      445|192.168.228.207|    6538| 7965|\n",
      "|192.168.229.154|      445|  192.168.227.2|   59095| 7965|\n",
      "| 192.168.226.67|      445|  192.168.225.1|   40372| 7965|\n",
      "| 192.168.229.92|      445|  192.168.226.8|   49851| 8737|\n",
      "+---------------+---------+---------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflow_v1.select('dest_ip', 'dest_port', 'src_ip', 'src_port', \"bytes\").where('dest_port = 445').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1  = netflow_v1.groupBy('dest_ip').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "192.228.79.201\n",
    "156.154.145.182\n",
    "37.187.37.150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set bad ip addresses to 1 and all other 0\n",
    "netflow_v1 = netflow_v1.select(col('*'),fn.expr(\"case when dest_ip = '40.80.148.42' OR src_ip = '40.80.148.42' OR dest_ip = '23.22.63.114' OR src_ip = '23.22.63.114' OR dest_ip = '156.154.145.182' OR src_ip = '156.154.145.182' OR dest_ip = '192.228.79.201' OR src_ip = '192.228.79.201' OR dest_ip = '37.187.37.150' OR src_ip = '37.187.37.150' then 1 else 0 end\").alias(\"bad_ip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1629"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.where('bad_ip = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'endtime', 'timestamp', 'bytes', 'src_ip', 'src_port', 'bytes_in', 'dest_ip', 'dest_port', 'bytes_out', 'bad_ports', 'large_upload', 'z_score', 'z_score_alpha', 'bad_ip']\n"
     ]
    }
   ],
   "source": [
    "print(netflow_v1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import strptime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second, mean, unix_timestamp, substring, abs, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, TimestampType, DateType\n",
    "from pyspark.sql import functions as func\n",
    "from scipy import stats\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1 = netflow_v1_labeled.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    time_test = []\n",
    "    for i in range(0,len(x)):\n",
    "        time_test.append(re.sub('(.\\d{6}Z)', '', x[i]))\n",
    "        return time_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endtime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>bytes</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>bad_ports</th>\n",
       "      <th>large_upload</th>\n",
       "      <th>z_score</th>\n",
       "      <th>z_score_alpha</th>\n",
       "      <th>bad_ip</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>timestart</th>\n",
       "      <th>timeend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-28T235857.669491Z</td>\n",
       "      <td>2016-08-28T235857.649918Z</td>\n",
       "      <td>5978</td>\n",
       "      <td>192.168.228.105</td>\n",
       "      <td>37119</td>\n",
       "      <td>2463</td>\n",
       "      <td>192.168.224.78</td>\n",
       "      <td>8089</td>\n",
       "      <td>3515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03782356199534734</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235857</td>\n",
       "      <td>2016-08-28T235857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-28T235854.479746Z</td>\n",
       "      <td>2016-08-28T235854.464876Z</td>\n",
       "      <td>5377</td>\n",
       "      <td>192.168.225.88</td>\n",
       "      <td>47224</td>\n",
       "      <td>2092</td>\n",
       "      <td>192.168.228.88</td>\n",
       "      <td>8089</td>\n",
       "      <td>3285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03835168741366982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235854</td>\n",
       "      <td>2016-08-28T235854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     endtime                  timestamp  bytes  \\\n",
       "0  2016-08-28T235857.669491Z  2016-08-28T235857.649918Z   5978   \n",
       "1  2016-08-28T235854.479746Z  2016-08-28T235854.464876Z   5377   \n",
       "\n",
       "            src_ip  src_port  bytes_in         dest_ip  dest_port  bytes_out  \\\n",
       "0  192.168.228.105     37119      2463  192.168.224.78       8089       3515   \n",
       "1   192.168.225.88     47224      2092  192.168.228.88       8089       3285   \n",
       "\n",
       "   bad_ports  large_upload               z_score  z_score_alpha  bad_ip  \\\n",
       "0          0             0  -0.03782356199534734              0       0   \n",
       "1          0             0  -0.03835168741366982              0       0   \n",
       "\n",
       "   anomalous          timestart            timeend  \n",
       "0          0  2016-08-28T235857  2016-08-28T235857  \n",
       "1          0  2016-08-28T235854  2016-08-28T235854  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestart = []\n",
    "for i in range(0,len(netflow_v1)):\n",
    "    timestart.append(re.sub('(.\\d{6}Z)', '', netflow_v1['timestamp'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeend = []\n",
    "for i in range(0,len(netflow_v1)):\n",
    "    timeend.append(re.sub('(.\\d{6}Z)', '', netflow_v1['endtime'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1['timestart'] = timestart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1['timeend'] = timeend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-08-28T235857']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_time(netflow_v1['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endtime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>bytes</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>bad_ports</th>\n",
       "      <th>large_upload</th>\n",
       "      <th>z_score</th>\n",
       "      <th>z_score_alpha</th>\n",
       "      <th>bad_ip</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>timestart</th>\n",
       "      <th>timeend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-28T235857.669491Z</td>\n",
       "      <td>2016-08-28T235857.649918Z</td>\n",
       "      <td>5978</td>\n",
       "      <td>192.168.228.105</td>\n",
       "      <td>37119</td>\n",
       "      <td>2463</td>\n",
       "      <td>192.168.224.78</td>\n",
       "      <td>8089</td>\n",
       "      <td>3515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03782356199534734</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235857</td>\n",
       "      <td>2016-08-28T235857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     endtime                  timestamp  bytes  \\\n",
       "0  2016-08-28T235857.669491Z  2016-08-28T235857.649918Z   5978   \n",
       "\n",
       "            src_ip  src_port  bytes_in         dest_ip  dest_port  bytes_out  \\\n",
       "0  192.168.228.105     37119      2463  192.168.224.78       8089       3515   \n",
       "\n",
       "   bad_ports  large_upload               z_score  z_score_alpha  bad_ip  \\\n",
       "0          0             0  -0.03782356199534734              0       0   \n",
       "\n",
       "   anomalous          timestart            timeend  \n",
       "0          0  2016-08-28T235857  2016-08-28T235857  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting an user define function:\n",
    "# This function converts the string cell into a date:\n",
    "conv_date =  udf(lambda x: strptime(x, \"%Y-%m-%dT%H%M%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-08-28T235854'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1['timestart'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[bytes: bigint, src_ip: string, src_port: bigint, bytes_in: bigint, dest_ip: string, dest_port: bigint, bytes_out: bigint, bad_ports: bigint, large_upload: bigint, z_score: string, z_score_alpha: bigint, bad_ip: bigint, anomalous: bigint, timestart: string, timeend: string]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1_df.drop('timestamp', 'endtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_test = '2016-08-28T235857.649918Z', '2016-08-29T255857.649918Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for item in time_test:\n",
    "    time_test1 = re.sub('(.\\d{6}Z)', '', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-08-29T255857'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_object = [strptime(netflow_v1['timestart'][i], \"%Y-%m-%dT%H%M%S\") for i in range(0,len(netflow_v1))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_object_end = [strptime(netflow_v1['timeend'][i], \"%Y-%m-%dT%H%M%S\") for i in range(0,len(netflow_v1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(time_object[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import mktime\n",
    "\n",
    "time_start = [datetime.fromtimestamp(mktime(time_object[i])) for i in range(0, len(time_object))]\n",
    "time_end = [datetime.fromtimestamp(mktime(time_object_end[i])) for i in range(0, len(time_object_end))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_hour = [time_start[i].hour for i in range(0, len(time_start))]\n",
    "time_minute = [time_start[i].minute for i in range(0, len(time_start))]\n",
    "time_hour_end = [time_end[i].hour for i in range(0, len(time_start))]\n",
    "time_minute_end = [time_end[i].minute for i in range(0, len(time_start))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1['hour'] = time_hour\n",
    "netflow_v1['minute'] = time_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through dataframe... if private to private match, the return 0, else will return NaN \n",
    "netflow_v1.loc[(netflow_v1.dest_ip.str.contains('10.0|172.31|192.168')) & (netflow_v1.src_ip.str.contains('10.0|172.31|192.168')), 'private'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all values of NaN with 1 meaning 1 is not a private to private mapping\n",
    "netflow_v1['private'] = netflow_v1.private.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endtime</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>bytes</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>bad_ports</th>\n",
       "      <th>large_upload</th>\n",
       "      <th>z_score</th>\n",
       "      <th>z_score_alpha</th>\n",
       "      <th>bad_ip</th>\n",
       "      <th>anomalous</th>\n",
       "      <th>timestart</th>\n",
       "      <th>timeend</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-28T235857.669491Z</td>\n",
       "      <td>2016-08-28T235857.649918Z</td>\n",
       "      <td>5978</td>\n",
       "      <td>192.168.228.105</td>\n",
       "      <td>37119</td>\n",
       "      <td>2463</td>\n",
       "      <td>192.168.224.78</td>\n",
       "      <td>8089</td>\n",
       "      <td>3515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03782356199534734</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235857</td>\n",
       "      <td>2016-08-28T235857</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-28T235854.479746Z</td>\n",
       "      <td>2016-08-28T235854.464876Z</td>\n",
       "      <td>5377</td>\n",
       "      <td>192.168.225.88</td>\n",
       "      <td>47224</td>\n",
       "      <td>2092</td>\n",
       "      <td>192.168.228.88</td>\n",
       "      <td>8089</td>\n",
       "      <td>3285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03835168741366982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235854</td>\n",
       "      <td>2016-08-28T235854</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-28T235852.503557Z</td>\n",
       "      <td>2016-08-28T235852.512410Z</td>\n",
       "      <td>9333</td>\n",
       "      <td>192.168.225.234</td>\n",
       "      <td>25022</td>\n",
       "      <td>1579</td>\n",
       "      <td>23.203.184.161</td>\n",
       "      <td>443</td>\n",
       "      <td>7754</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03487537434397975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235852</td>\n",
       "      <td>2016-08-28T235852</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-28T235851.631702Z</td>\n",
       "      <td>2016-08-28T235851.610489Z</td>\n",
       "      <td>6723</td>\n",
       "      <td>192.168.228.69</td>\n",
       "      <td>55949</td>\n",
       "      <td>2994</td>\n",
       "      <td>192.168.229.165</td>\n",
       "      <td>8089</td>\n",
       "      <td>3729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.037168897375297015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235851</td>\n",
       "      <td>2016-08-28T235851</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-28T235849.106067Z</td>\n",
       "      <td>2016-08-28T235849.091685Z</td>\n",
       "      <td>6010</td>\n",
       "      <td>192.168.226.142</td>\n",
       "      <td>3322</td>\n",
       "      <td>2463</td>\n",
       "      <td>192.168.224.47</td>\n",
       "      <td>8089</td>\n",
       "      <td>3547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.03779544217274115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-28T235849</td>\n",
       "      <td>2016-08-28T235849</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     endtime                  timestamp  bytes  \\\n",
       "0  2016-08-28T235857.669491Z  2016-08-28T235857.649918Z   5978   \n",
       "1  2016-08-28T235854.479746Z  2016-08-28T235854.464876Z   5377   \n",
       "2  2016-08-28T235852.503557Z  2016-08-28T235852.512410Z   9333   \n",
       "3  2016-08-28T235851.631702Z  2016-08-28T235851.610489Z   6723   \n",
       "4  2016-08-28T235849.106067Z  2016-08-28T235849.091685Z   6010   \n",
       "\n",
       "            src_ip  src_port  bytes_in          dest_ip  dest_port  bytes_out  \\\n",
       "0  192.168.228.105     37119      2463   192.168.224.78       8089       3515   \n",
       "1   192.168.225.88     47224      2092   192.168.228.88       8089       3285   \n",
       "2  192.168.225.234     25022      1579   23.203.184.161        443       7754   \n",
       "3   192.168.228.69     55949      2994  192.168.229.165       8089       3729   \n",
       "4  192.168.226.142      3322      2463   192.168.224.47       8089       3547   \n",
       "\n",
       "   bad_ports  large_upload                z_score  z_score_alpha  bad_ip  \\\n",
       "0          0             0   -0.03782356199534734              0       0   \n",
       "1          0             0   -0.03835168741366982              0       0   \n",
       "2          0             0   -0.03487537434397975              0       0   \n",
       "3          0             0  -0.037168897375297015              0       0   \n",
       "4          0             0   -0.03779544217274115              0       0   \n",
       "\n",
       "   anomalous          timestart            timeend  hour  minute  private  \n",
       "0          0  2016-08-28T235857  2016-08-28T235857    23      58      0.0  \n",
       "1          0  2016-08-28T235854  2016-08-28T235854    23      58      0.0  \n",
       "2          0  2016-08-28T235852  2016-08-28T235852    23      58      1.0  \n",
       "3          0  2016-08-28T235851  2016-08-28T235851    23      58      0.0  \n",
       "4          0  2016-08-28T235849  2016-08-28T235849    23      58      0.0  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = netflow_v1_df.select('timestart', unix_timestamp('timestart', \"yyyy-MM-ddTHHMMSS\") .cast(TimestampType()).alias(\"timestamp\"))\n",
    "test = netflow_v1_df.withColumn('timestamp', conv_date(netflow_v1_df.timestart))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Dataframe as a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2672"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflow_v1_labeled = netflow_v1.select(col(\"*\"), fn.expr(\"case when z_score_alpha == 1 OR large_upload == 1 OR bad_ports = 1 OR bad_ip = 1 then 1 else 0 end\").alias(\"anomalous\"))\n",
    "netflow_v1_labeled.where('anomalous == 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1_labeled = netflow_v1_labeled.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflow_v1_labeled.toPandas().to_csv('netflow_v1_labeled.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
